{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089ee5c8",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced NLP QA Project — From Scratch to SOTA (T5 & DistilBERT)\n",
    "**Goal:** Upgrade a scratch Transformer QA notebook into an **advanced-level NLP project** featuring modern tokenization, pretrained models, rigorous evaluation, decoding strategies, interpretability, and a live demo.\n",
    "\n",
    "**Highlights**\n",
    "- Subword tokenization (BPE/WordPiece) with Hugging Face tokenizers\n",
    "- Fine-tuning **T5-small** (generative QA) on custom QA CSV or SQuAD\n",
    "- Fine-tuning **DistilBERT** (extractive QA) on SQuAD with EM/F1\n",
    "- Metrics: **BLEU (sacrebleu), ROUGE**, **EM/F1** (SQuAD)\n",
    "- Decoding: greedy, **beam search**, **top‑k**, **nucleus (top‑p)**\n",
    "- **Attention visualization** for interpretability\n",
    "- **Gradio demo** for interactive inference\n",
    "- (Optional) **FastAPI** service stub for production deployment\n",
    "\n",
    "> You can run **either** on your local QA CSV (2 columns: `question`, `answer`) **or** on SQuAD via `datasets`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aad31",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fcdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running locally, uncomment to install dependencies.\n",
    "# Note: In some environments, you may need to restart the kernel after installation.\n",
    "\n",
    "# !pip install -U transformers datasets accelerate evaluate sacrebleu rouge-score gradio matplotlib torch torchvision torchaudio --quiet\n",
    "# For SQuAD EM/F1 utility\n",
    "# !pip install -U seqeval --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62f655",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d60c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5TokenizerFast,\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e778b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data — Use Custom CSV or SQuAD\n",
    "Choose one of the two paths below:\n",
    "\n",
    "- **Custom CSV** (two columns: `question`, `answer`) — set `CSV_PATH`.\n",
    "- **SQuAD** — set `USE_SQUAD=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Config ----\n",
    "USE_SQUAD = False  # set True to use SQuAD automatically\n",
    "CSV_PATH = 'data/reminiscences_of_a_stock_operator_qa.csv'  # your local QA CSV\n",
    "CSV_SEP = '\\t'  # change if comma-separated\n",
    "VAL_SPLIT = 0.1  # validation split for custom CSV\n",
    "MAX_TRAIN_SAMPLES = None  # set an int to subsample for faster experiments\n",
    "\n",
    "# Model names\n",
    "T5_MODEL = \"t5-small\"  # generative\n",
    "EXTRACTIVE_MODEL = \"distilbert-base-uncased\"  # extractive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_custom_csv(path: str, sep: str='\\t', val_split: float=0.1) -> DatasetDict:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    assert {'question','answer'}.issubset(df.columns), \"CSV must have 'question' and 'answer' columns.\"\n",
    "    if MAX_TRAIN_SAMPLES:\n",
    "        df = df.sample(min(MAX_TRAIN_SAMPLES, len(df)), random_state=SEED)\n",
    "    # Split\n",
    "    val_size = max(1, int(len(df)*val_split))\n",
    "    df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    df_train = df.iloc[:-val_size].reset_index(drop=True)\n",
    "    df_val = df.iloc[-val_size:].reset_index(drop=True)\n",
    "    # Convert to HF datasets\n",
    "    ds_train = HFDataset.from_pandas(df_train)\n",
    "    ds_val = HFDataset.from_pandas(df_val)\n",
    "    return DatasetDict(train=ds_train, validation=ds_val)\n",
    "\n",
    "if USE_SQUAD:\n",
    "    # Load SQuAD for both generative and extractive tracks\n",
    "    ds_squad = load_dataset(\"squad\")\n",
    "    if MAX_TRAIN_SAMPLES:\n",
    "        ds_squad = ds_squad.shuffle(seed=SEED)\n",
    "        ds_squad = DatasetDict(\n",
    "            train = ds_squad[\"train\"].select(range(min(MAX_TRAIN_SAMPLES, len(ds_squad[\"train\"])))),\n",
    "            validation = ds_squad[\"validation\"].select(range(min(MAX_TRAIN_SAMPLES//10 if MAX_TRAIN_SAMPLES else 1000, len(ds_squad[\"validation\"]))))\n",
    "        )\n",
    "else:\n",
    "    ds_custom = load_custom_csv(CSV_PATH, sep=CSV_SEP, val_split=VAL_SPLIT)\n",
    "\n",
    "print(\"Datasets ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca8ab9",
   "metadata": {},
   "source": [
    "## 3. Generative QA — T5-small (subword/BPE tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ddcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t5_tokenizer = T5TokenizerFast.from_pretrained(T5_MODEL)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL).to(device)\n",
    "\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 64\n",
    "\n",
    "def format_examples_for_t5(batch):\n",
    "    # For SQuAD, include context; for custom CSV, question->answer only\n",
    "    if USE_SQUAD:\n",
    "        inputs = [f\"question: {q}  context: {c}\" for q, c in zip(batch['question'], batch['context'])]\n",
    "        targets = batch['answers']\n",
    "        targets = [ans['text'][0] if len(ans['text'])>0 else \"\" for ans in targets]\n",
    "    else:\n",
    "        inputs = [f\"question: {q}\" for q in batch['question']]\n",
    "        targets = batch['answer']\n",
    "    model_inputs = t5_tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "    labels = t5_tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "if USE_SQUAD:\n",
    "    ds_t5 = ds_squad.map(format_examples_for_t5, batched=True, remove_columns=ds_squad['train'].column_names)\n",
    "else:\n",
    "    ds_t5 = ds_custom.map(format_examples_for_t5, batched=True, remove_columns=ds_custom['train'].column_names)\n",
    "\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8215400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir_t5 = \"t5-gen-qa\"\n",
    "\n",
    "args_t5 = TrainingArguments(\n",
    "    output_dir=output_dir_t5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_bleu_rouge(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "    decoded_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bleu = evaluate.load(\"sacrebleu\").compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    rouge = evaluate.load(\"rouge\").compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu[\"score\"], **{f\"rouge_{k}\": v for k, v in rouge.items()}}\n",
    "\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=args_t5,\n",
    "    train_dataset=ds_t5['train'],\n",
    "    eval_dataset=ds_t5['validation'],\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator_t5,\n",
    "    compute_metrics=compute_bleu_rouge\n",
    ")\n",
    "\n",
    "# To train: uncomment the next line\n",
    "# trainer_t5.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a19cd0",
   "metadata": {},
   "source": [
    "### Decoding: greedy, beam search, top‑k, top‑p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995aa449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def t5_generate(text_prompt: str, \n",
    "                 max_new_tokens: int=64,\n",
    "                 decoding: str=\"greedy\",\n",
    "                 num_beams: int=4,\n",
    "                 top_k: int=50,\n",
    "                 top_p: float=0.95,\n",
    "                 temperature: float=1.0):\n",
    "    inputs = t5_tokenizer(text_prompt, return_tensors=\"pt\").to(device)\n",
    "    gen_kwargs = dict(max_new_tokens=max_new_tokens)\n",
    "    if decoding == \"greedy\":\n",
    "        pass\n",
    "    elif decoding == \"beam\":\n",
    "        gen_kwargs.update(dict(num_beams=num_beams, early_stopping=True))\n",
    "    elif decoding == \"topk\":\n",
    "        gen_kwargs.update(dict(do_sample=True, top_k=top_k, temperature=temperature))\n",
    "    elif decoding == \"topp\":\n",
    "        gen_kwargs.update(dict(do_sample=True, top_p=top_p, temperature=temperature))\n",
    "    else:\n",
    "        raise ValueError(\"decoding must be one of: greedy | beam | topk | topp\")\n",
    "    outputs = t5_model.generate(**inputs, **gen_kwargs)\n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example (after training or with base T5):\n",
    "# print(t5_generate(\"question: What is a stop-loss order?\", decoding=\"beam\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9924bde",
   "metadata": {},
   "source": [
    "### Attention Visualization (Encoder Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable attentions\n",
    "t5_model.config.output_attentions = True\n",
    "\n",
    "def visualize_attention(prompt: str, layer: int=0, head: int=0):\n",
    "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.encoder(**inputs, output_attentions=True, return_dict=True)\n",
    "    attentions = outputs.attentions  # tuple: (layers) x (batch, heads, seq_len, seq_len)\n",
    "    attn = attentions[layer][0, head].detach().cpu().numpy()\n",
    "    tokens = t5_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(attn)\n",
    "    plt.title(f\"Encoder Layer {layer} Head {head}\")\n",
    "    plt.xlabel(\"Keys\")\n",
    "    plt.ylabel(\"Queries\")\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# visualize_attention(\"question: Explain diversification in portfolio management.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8887634",
   "metadata": {},
   "source": [
    "## 4. Extractive QA — DistilBERT on SQuAD (EM/F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dcdfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_SQUAD:\n",
    "    tokenizer_ex = AutoTokenizer.from_pretrained(EXTRACTIVE_MODEL, use_fast=True)\n",
    "    model_ex = AutoModelForQuestionAnswering.from_pretrained(EXTRACTIVE_MODEL).to(device)\n",
    "\n",
    "    max_length = 384\n",
    "    doc_stride = 128\n",
    "\n",
    "    def prepare_train_features(examples):\n",
    "        tokenized_examples = tokenizer_ex(\n",
    "            examples[\"question\"],\n",
    "            examples[\"context\"],\n",
    "            truncation=\"only_second\",\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer_ex.cls_token_id) if tokenizer_ex.cls_token_id in input_ids else 0\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[\"answers\"][sample_index]\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "                continue\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "        tokenized_examples[\"start_positions\"] = start_positions\n",
    "        tokenized_examples[\"end_positions\"] = end_positions\n",
    "        return tokenized_examples\n",
    "\n",
    "    def prepare_validation_features(examples):\n",
    "        tokenized_examples = tokenizer_ex(\n",
    "            examples[\"question\"],\n",
    "            examples[\"context\"],\n",
    "            truncation=\"only_second\",\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "        return tokenized_examples\n",
    "\n",
    "    squad_tokenized_train = ds_squad[\"train\"].map(prepare_train_features, batched=True, remove_columns=ds_squad[\"train\"].column_names)\n",
    "    squad_tokenized_val = ds_squad[\"validation\"].map(prepare_validation_features, batched=True, remove_columns=ds_squad[\"validation\"].column_names)\n",
    "\n",
    "    args_ex = TrainingArguments(\n",
    "        output_dir=\"distilbert-extractive-qa\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size=12,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50\n",
    "    )\n",
    "\n",
    "    data_collator_ex = None  # default is fine for QA\n",
    "\n",
    "    trainer_ex = Trainer(\n",
    "        model=model_ex,\n",
    "        args=args_ex,\n",
    "        train_dataset=squad_tokenized_train,\n",
    "        eval_dataset=squad_tokenized_val,\n",
    "        tokenizer=tokenizer_ex,\n",
    "        data_collator=data_collator_ex,\n",
    "    )\n",
    "\n",
    "    # To train: uncomment\n",
    "    # trainer_ex.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26622072",
   "metadata": {},
   "source": [
    "### EM / F1 Evaluation (SQuAD-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f82d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(s):\n",
    "    import re, string\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "def compute_em_f1(prediction: str, ground_truth: str):\n",
    "    pred_tokens = get_tokens(prediction)\n",
    "    gt_tokens = get_tokens(ground_truth)\n",
    "    common = set(pred_tokens) & set(gt_tokens)\n",
    "    num_same = sum(min(pred_tokens.count(w), gt_tokens.count(w)) for w in common)\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return int(pred_tokens == gt_tokens), 0\n",
    "    if num_same == 0:\n",
    "        return 0, 0\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gt_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    em = int(normalize_text(prediction) == normalize_text(ground_truth))\n",
    "    return em, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead290d",
   "metadata": {},
   "source": [
    "### Validation Loop for T5 (BLEU/ROUGE + optional EM/F1 if ground truth provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_t5(ds_eval, sample_size: Optional[int]=64):\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    preds, refs = [], []\n",
    "    ems, f1s = [], []\n",
    "    n = len(ds_eval) if sample_size is None else min(sample_size, len(ds_eval))\n",
    "    for i in range(n):\n",
    "        ex = ds_eval[i]\n",
    "        if USE_SQUAD:\n",
    "            q = ex['question']; c = ex['context']; refs_i = ex['answers']['text']\n",
    "            inp = f\"question: {q}  context: {c}\"\n",
    "            gold = refs_i[0] if len(refs_i)>0 else \"\"\n",
    "        else:\n",
    "            q = ex['question']; gold = ex['answer']\n",
    "            inp = f\"question: {q}\"\n",
    "        pred = t5_generate(inp, decoding=\"beam\", num_beams=4)\n",
    "        preds.append(pred); refs.append([gold])\n",
    "        if gold is not None:\n",
    "            em, f1 = compute_em_f1(pred, gold)\n",
    "            ems.append(em); f1s.append(f1)\n",
    "    bleu = sacrebleu.compute(predictions=preds, references=refs)\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=[r[0] for r in refs])\n",
    "    out = {\"bleu\": bleu[\"score\"], **{f\"rouge_{k}\": v for k, v in rouge_scores.items()}}\n",
    "    if ems:\n",
    "        out.update({\"em\": float(np.mean(ems)), \"f1\": float(np.mean(f1s))})\n",
    "    return out\n",
    "\n",
    "# Example after training:\n",
    "# results = evaluate_t5(ds_t5['validation'], sample_size=64)\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7a0f0",
   "metadata": {},
   "source": [
    "## 5. Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "def t5_answer_fn(question: str, context: str=\"\"):\n",
    "    prompt = f\"question: {question}\" + (f\"  context: {context}\" if context else \"\")\n",
    "    return t5_generate(prompt, decoding=\"beam\", num_beams=4)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=t5_answer_fn,\n",
    "    inputs=[gr.Textbox(label=\"Question\"), gr.Textbox(label=\"Context (optional)\")]\n",
    "    ,\n",
    "    outputs=\"text\",\n",
    "    title=\"Generative QA (T5-small)\",\n",
    "    examples=[\n",
    "        [\"What is diversification?\", \"\"],\n",
    "        [\"Who wrote the book 'Reminiscences of a Stock Operator'?\", \"\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# To launch locally: uncomment\n",
    "# demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8aa33",
   "metadata": {},
   "source": [
    "## 6. Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38081578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_t5(path=\"t5-gen-qa-final\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    t5_model.save_pretrained(path)\n",
    "    t5_tokenizer.save_pretrained(path)\n",
    "    print(\"Saved to\", path)\n",
    "\n",
    "def load_t5(path=\"t5-gen-qa-final\"):\n",
    "    global t5_model, t5_tokenizer\n",
    "    t5_tokenizer = T5TokenizerFast.from_pretrained(path)\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(path).to(device)\n",
    "    print(\"Loaded from\", path)\n",
    "\n",
    "# Example:\n",
    "# save_t5()\n",
    "# load_t5()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf01f33",
   "metadata": {},
   "source": [
    "## 7. (Optional) FastAPI Service Stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c25a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save this to fastapi_app.py if you want a production microservice.\n",
    "fastapi_code = r'''from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "app = FastAPI()\n",
    "MODEL_PATH = \"t5-gen-qa-final\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "\n",
    "class QARequest(BaseModel):\n",
    "    question: str\n",
    "    context: str = \"\"\n",
    "\n",
    "@app.post(\"/qa\")\n",
    "def qa(req: QARequest):\n",
    "    prompt = f\"question: {req.question}\" + (f\"  context: {req.context}\" if req.context else \"\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, num_beams=4, early_stopping=True)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"answer\": answer}\n",
    "'''\n",
    "with open(\"fastapi_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(fastapi_code)\n",
    "print(\"Wrote fastapi_app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73be41b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### How to Use\n",
    "1. Set `USE_SQUAD=True` **or** point `CSV_PATH` to your QA CSV.\n",
    "2. Run **Sections 0→7** in order.\n",
    "3. Train T5 (uncomment `trainer_t5.train()`).\n",
    "4. (Optional) Train DistilBERT on SQuAD (set `USE_SQUAD=True` and uncomment `trainer_ex.train()`).\n",
    "5. Evaluate with `evaluate_t5(...)` and record **BLEU/ROUGE/EM/F1**.\n",
    "6. Save model with `save_t5()`, then run **Gradio demo** or deploy via **FastAPI**.\n",
    "\n",
    "**Deliverables to show on a resume**\n",
    "- Comparison table: **scratch Transformer vs. T5 vs. DistilBERT** on your dataset/SQuAD.\n",
    "- Plots of metrics vs. epochs.\n",
    "- Screenshots/GIF of the **Gradio** app.\n",
    "- Short write-up on decoding strategies & attention visualization findings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
